{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0baecdb",
   "metadata": {},
   "source": [
    "## Chapter 7: Policy Gradient Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a0b46c4",
   "metadata": {},
   "source": [
    "### Policy-based Reinforcement Learning\n",
    "\n",
    "In previous chapters, we focused on learning value function and generate policy from it. Yet there are RL algorithms that directly generate the policy from experience: **Policy-based RL**.\n",
    "\n",
    "In policy-based RL, policy is parametrized by differentiable parameter $\\theta$. \n",
    "</br>\n",
    "</br>\n",
    "<font size=\"3\">\n",
    "$$\\begin{align}\n",
    "\\pi_\\theta(a|s) = \\mathbb{P}[a |s, \\theta]\n",
    "\\end{align}$$\n",
    "</font>\n",
    "\n",
    "Policy-based RL is effective in high-dimensional or continuous action spaces, and able to learn stochastic policies. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2dcbef9",
   "metadata": {},
   "source": [
    "e"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "093d6294",
   "metadata": {},
   "source": [
    "### Monte-Carlo Policy Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "189a03e4",
   "metadata": {},
   "source": [
    "### Actor-Critic Policy Gradient"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl",
   "language": "python",
   "name": "rl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
