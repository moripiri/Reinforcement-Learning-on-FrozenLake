{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da7fe698",
   "metadata": {},
   "source": [
    "## Chapter 4: Model-Free Prediction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c487832a",
   "metadata": {},
   "source": [
    "Compared to **Dynamics Programming**, **Model-free prediction** algorithms aim to estimate the **value function of a certain policy** *without knowing the MDP*. Mainly, **Monte-Carlo Prediction** and **Temporal-Difference learning (TD-learning)** are known as a **model-free** prediction algorithms. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "966641ff",
   "metadata": {},
   "source": [
    "### Monte-Carlo Prediction\n",
    "\n",
    "According to Wikipedia, **Monte-Carlo (MC)** method is a broad class of algorithms that repeated random sampling to obtain numerical results. For example, the value of $\\pi$ can be approximated using a Monte Carlo method, by uniformly scattering number of points over the square. \n",
    "\n",
    "In Reinforcement Learning, it can be used to evaulate the value function of the policy $\\pi$ by sampling return of multiple episodes.\n",
    "</br>\n",
    "</br>\n",
    "<font size=\"4\">\n",
    "$$\\begin{align}\n",
    "v_\\pi(s) = \\mathbb{E}_\\pi[G_t |S_t = s]\n",
    "\\end{align}$$\n",
    "</font>\n",
    "where return $G_t = R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\cdots \\gamma^{T-1} R_T$. \n",
    "\n",
    "Monte-Carlo policy evaluation consider empirical mean return as value. \n",
    "\n",
    "After an episode end, we can update the value of the states visited in the episode. Value of the states can be updated by average of the returns following the first visits to the states (*First-Visit* MC), or every visits to the states (*Every-Visit* MC). \n",
    "\n",
    "For example, we can record **list of total returns $G_t$** by the first timestep, or every timestep $t$ that state $s$ is visited in an episode, and estimate its value by average of returns.  \n",
    "\n",
    "\n",
    "Both first-visit and every-visit MC converge to true value function $v_\\pi$ with finite variance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f77f3225",
   "metadata": {},
   "source": [
    "### Incremental Monte-Carlo Updates\n",
    "\n",
    "Monte-Carlo prediction requires list of total returns in order to calculate average of returns. \n",
    "However, recording the history of total returns for every states can be avoided by implementing incremental mean.\n",
    "</br>\n",
    "</br>\n",
    "<font size=\"4\">\n",
    "$$\\begin{align}\n",
    "\\mu_k = \\frac{1}{k} \\sum_{j=1}^k x_j = \\frac{1}{k}(x_k + \\sum_{j=1}^{k-1} x_j) & \\\\ =\\frac{1}{k}(x_k + (k-1)\\mu_{k-1}) \\\\ = \\mu_{k-1} + \\frac{1}{k}(x_k - \\mu_{k-1})\n",
    "\\end{align}$$\n",
    "</font>\n",
    "\n",
    "Monte-Carlo value $V(s)$ can be updated by using incremental mean as following. \n",
    "</br>\n",
    "</br>\n",
    "<font size=\"4\">\n",
    "$$\\begin{align}\n",
    "V(S_t) = V(S_t) + \\frac{1}{N(S_t)}(G_t - V(S_t))\n",
    "\\end{align}$$\n",
    "</font>\n",
    "where $N(S_t)$ is a number of times $V(S_t)$ is updated. \n",
    "\n",
    "In non-stationary problems, $\\frac{1}{N(S_t)}$ is usually replaced to constant hyperparameter $\\alpha \\in [0, 1]$ to keep track of a runnning mean. \n",
    "</br>\n",
    "</br>\n",
    "<font size=\"4\">\n",
    "$$\\begin{align}\n",
    "V(S_t) = V(S_t) + \\alpha(G_t - V(S_t))\n",
    "\\end{align}$$\n",
    "</font>\n",
    "\n",
    "In this case, which is called constant-$\\alpha$ MC, hyperparameter $\\alpha$ acts as a step-size for MC value update. If $\\alpha = 0$, $V(S_t)$ will not be updated by the policy, and if $\\alpha = 1$, $V(S_t)$ is equal to the last return experienced by the policy.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c5e998",
   "metadata": {},
   "source": [
    "### Temporal-Difference Learning\n",
    "\n",
    "**Temporal-Difference (TD)** learning is a model-free learning algorithm that learns by bootstrapping from the current estimate of the value function. \n",
    "\n",
    "</br>\n",
    "</br>\n",
    "<font size=\"4\">\n",
    "$$\\begin{align}\n",
    "v_\\pi(s) = \\mathbb{E}_\\pi[G_t |S_t = s]  & \\\\ = \\mathbb{E}_\\pi[\\sum_{k=0}^{T-1}\\gamma^k R_{t+k+1} |S_t = s] \\\\= \\mathbb{E}_\\pi[R_{t+1} + \\gamma \\sum_{k=0}^{T-2}\\gamma^{k} R_{t+k+2} |S_t = s] \\\\=\n",
    "\\mathbb{E}_\\pi [R_{t+1} + \\gamma v_\\pi(s_{t+1}) | S_t = s]\n",
    "\\end{align}$$\n",
    "</font>\n",
    "\n",
    "Temporal-Difference learning updates value $V(S_t)$ towards estimated return $R_{t+1} + \\gamma V(S_{t+1})$.\n",
    "</br>\n",
    "</br>\n",
    "<font size=\"4\">\n",
    "$$\\begin{align}\n",
    "V(S_t) = V(S_t) + \\alpha(R_{t+1} + \\gamma V(S_{t+1}) - V(S_t))\n",
    "\\end{align}$$\n",
    "</font>\n",
    "\n",
    "$R_{t+1} + \\gamma V(S_{t+1})$ is often called the **TD target**, and $\\delta_t = R_{t+1} + \\gamma V(S_{t+1}) - V(S_t)$ as the **TD error**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dbaf76d",
   "metadata": {},
   "source": [
    "### Difference between Monte-Carlo (MC) and Temporal-Difference (TD) \n",
    "\n",
    "Even though MC and TD are both model-free learning algorithms, they have many differences.\n",
    "\n",
    "- **MC** must wait until the episode ends to calculate return $\\longleftrightarrow$  **TD** can learn before the episode ends \n",
    "-  **Return** $G_t = R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\cdots \\gamma^{T-1} R_T$ is **unbiased** estimate of $v_\\pi(S_t)$ $\\longleftrightarrow$ **TD target** $R_{t+1} + \\gamma V(S_{t+1})$ is **biased** estimate of $v_\\pi(S_t)$\n",
    "- **Return** has **high variance** $\\longleftrightarrow$ **TD target** has much **lower variance** than return\n",
    "- High variance and zero bias makes **MC** insensitive to initial value $\\longleftrightarrow$ Low variance and some bias makes **TD** more efficient than **MC**, but also more sensitive to initial value \n",
    "- **MC** does not bootstrap and exploit Markov property $\\longleftrightarrow$ **TD** bootstraps and exploits Markov property"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65c703b5",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1240a6dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl",
   "language": "python",
   "name": "rl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
