{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 2: Markov Decision Processes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Markov Decision Process (MDP)** is an environment that can be defined as 5-tuple:\n",
    "</br>\n",
    "</br>\n",
    "<font size=\"4\">\n",
    "$$\\begin{align}\n",
    "(\\mathcal{S}, \\mathcal{A}, \\mathcal{P}, \\mathcal{R}, \\gamma)\n",
    "\\end{align}$$\n",
    "</font>\n",
    "where\n",
    "\n",
    "- $\\mathcal{S}$: state space (set of states)\n",
    "\n",
    "- $\\mathcal{A}$: action space (set of actions)\n",
    "\n",
    "- $\\mathcal{P}$: state transition probability matrix $p(s'|s, a) = \\mathbb{P}[S_{t+1}=s'|S_t=s, A_t=a]$\n",
    "\n",
    "- $\\mathcal{R}$: reward function $r(s, a) = \\mathbb{E}[R_{t+1}|S_t=s, A_t=a]$\n",
    "\n",
    "- $\\gamma$: discount factor $\\gamma \\in [0, 1]$\n",
    "\n",
    "\n",
    "state space $\\mathcal{S}$ and action space $\\mathcal{A}$ can be infinite, but they are usually assumed to be finite in most RL theories. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Markov Property\n",
    "\n",
    "In State space $\\mathcal{S}$, state $S_{t+1} \\in \\mathcal{S}$ is **independent of past given only the current state** $S_t$. This is called **Markov property**. It can be written as \n",
    "</br>\n",
    "</br>\n",
    "<font size=\"4\">\n",
    "$$\\begin{align}\n",
    "\\mathbb{P}[S_{t+1}|S_t] = \\mathbb{P}[S_{t+1}|S_1, S_2, ... , S_t]\n",
    "\\end{align}$$\n",
    "</font>\n",
    "\n",
    "Thus, only knowing current state $S_{t}$ is enough for RL, if the environment satisfies the markov property.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Return\n",
    "\n",
    "In RL, reward function $r(s, a)$ is defined as mean value of rewards in state $s$ and action $a$. However, only considering the reward of next timestep may not be wise. To make agent consider reward of the far future too, **Return** is widely used.\n",
    "</br>\n",
    "</br>\n",
    "<font size=\"4\">\n",
    "$$\\begin{align}\n",
    "G_t = R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} \\cdots  = \\sum_{k=0}^\\infty \\gamma^k R_{t+k+1}\n",
    "\\end{align}$$\n",
    "</font>\n",
    "\n",
    "**Return** is the total discounted reward from timestep $t$.</br>\n",
    "The discount $\\gamma \\in [0, 1]$ affects the present value of the future rewards.</br>\n",
    "Reward $R$ after timestep $t+1$ is discounted to $\\gamma^k R$ by $\\gamma \\in [0, 1]$ in current timestep. \n",
    "</br></br>\n",
    "$\\gamma$ can be set between 0 and 1. $\\gamma$ close to 0 makes agent more near-sighted to rewards, and $\\gamma$ close to 1 makes it vice-versa.   \n",
    "</br>\n",
    "$\\gamma$ is usually set as 0.9 or 0.99 but not 1 as it means it can result infinite returns in cylic MDP.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Value function & Action-value function (Q-function)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
