{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 2: Markov Decision Processes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Markov Decision Process (MDP)** is an environment that can be defined as 5-tuple:\n",
    "</br>\n",
    "</br>\n",
    "<font size=\"4\">\n",
    "$$\\begin{align}\n",
    "(\\mathcal{S}, \\mathcal{A}, \\mathcal{P}, \\mathcal{R}, \\gamma)\n",
    "\\end{align}$$\n",
    "</font>\n",
    "where\n",
    "\n",
    "- $\\mathcal{S}$: state space (set of states)\n",
    "\n",
    "- $\\mathcal{A}$: action space (set of actions)\n",
    "\n",
    "- $\\mathcal{P}$: state transition probability matrix $\\mathcal{P}_{ss'}^a = \\mathbb{P}[S_{t+1}=s'|S_t=s, A_t=a]$\n",
    "\n",
    "- $\\mathcal{R}$: reward function $\\mathcal{R}_s^a = \\mathbb{E}[R_{t+1}|S_t=s, A_t=a]$\n",
    "\n",
    "- $\\gamma$: discount factor $\\gamma \\in [0, 1]$\n",
    "\n",
    "\n",
    "state space $\\mathcal{S}$ and action space $\\mathcal{A}$ can be infinite, but they are usually assumed to be finite in most RL theories. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Markov Property\n",
    "\n",
    "In State space $\\mathcal{S}$, state $S_{t+1} \\in \\mathcal{S}$ is **independent of past given only the current state** $S_t$. This is called **Markov property**. It can be written as \n",
    "</br>\n",
    "</br>\n",
    "<font size=\"4\">\n",
    "$$\\begin{align}\n",
    "\\mathbb{P}[S_{t+1}|S_t] = \\mathbb{P}[S_{t+1}|S_1, S_2, ... , S_t]\n",
    "\\end{align}$$\n",
    "</font>\n",
    "\n",
    "Thus, only knowing current state $S_{t}$ is enough for RL, if the environment satisfies the markov property.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy\n",
    "\n",
    "In Markov Decision Process, the agent need to decide what action to take in a given state. Such distribution over action for states is called **Policy**.\n",
    "</br>\n",
    "</br>\n",
    "<font size=\"4\">\n",
    "$$\\begin{align}\n",
    "\\pi(a|s) = \\mathbb{P}[A_{t} = a|S_t = s]\n",
    "\\end{align}$$\n",
    "</font>\n",
    "\n",
    "As the state is Markov, the policy considers only current state $S_t$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Return\n",
    "\n",
    "In RL, reward function $r(s, a)$ is defined as mean value of rewards in state $s$ and action $a$. However, only considering the reward of next timestep may not be wise. To make agent consider reward of the far future too, **Return** is widely used.\n",
    "</br>\n",
    "</br>\n",
    "<font size=\"4\">\n",
    "$$\\begin{align}\n",
    "G_t = R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} \\cdots  = \\sum_{k=0}^\\infty \\gamma^k R_{t+k+1}\n",
    "\\end{align}$$\n",
    "</font>\n",
    "\n",
    "**Return** is the total discounted reward from timestep $t$.</br>\n",
    "The discount $\\gamma \\in [0, 1]$ affects the present value of the future rewards.</br>\n",
    "Reward $R$ after timestep $t+1$ is discounted to $\\gamma^k R$ by $\\gamma \\in [0, 1]$ in current timestep. \n",
    "</br></br>\n",
    "$\\gamma$ can be set between 0 and 1. $\\gamma$ close to 0 makes agent more near-sighted to rewards, and $\\gamma$ close to 1 makes it vice-versa.   \n",
    "</br>\n",
    "$\\gamma$ is usually set as 0.9 or 0.99 but not 1 as it means it can result infinite returns in cylic MDP.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Value function & Action-value function (Q-value function)\n",
    "\n",
    "Almost all RL algorithms estimate value of states in environment - how good is a state for the agent.  \n",
    "In RL, 'how good' is estimated by **return**, and expected return in state $s$ under policy $\\pi$ is called **value function** $v_\\pi (s)$.\n",
    "</br>\n",
    "</br>\n",
    "<font size=\"4\">\n",
    "$$\\begin{align}\n",
    "v_\\pi(s) = \\mathbb{E}_\\pi[G_t|S_t = s]\n",
    "\\end{align}$$\n",
    "</font>\n",
    "\n",
    "We can also think of the value of taking action $a$ in state $s$ under policy $\\pi$. This is called **action-value function**, or Q-value $q_\\pi(s, a)$.\n",
    "</br>\n",
    "</br>\n",
    "<font size=\"4\">\n",
    "$$\\begin{align}\n",
    "q_\\pi(s, a) = \\mathbb{E}_\\pi[G_t|S_t = s, A_t = a]\n",
    "\\end{align}$$\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bellman equation\n",
    "\n",
    "**Bellman equation**, which is one of the central elements of RL, expresses value and action-value can be decomposed into two parts:\n",
    "\n",
    "1. **immediate reward**\n",
    "2. **discounted future values**\n",
    "\n",
    "For example, value function $v_\\pi(s)$ is\n",
    "</br>\n",
    "</br>\n",
    "<font size=\"4\">\n",
    "$$\\begin{align}\n",
    "v_\\pi(s) = \\mathbb{E}_\\pi[G_t|S_t = s] = \\mathbb{E}_\\pi[\\sum_{k=0}^\\infty \\gamma^k R_{t+k+1} | S_t = s]&\n",
    " \\\\= \\mathbb{E}_\\pi[R_{t+1} + \\gamma \\sum_{k=0}^\\infty \\gamma^k R_{t+k+2} | S_t = s] \\\\= \\mathbb{E}_\\pi[R_{t+1} + \\gamma v_\\pi(s_{t+1}) | S_t = s]\n",
    "\\end{align}$$\n",
    "</font>\n",
    "\n",
    "and similarly, action-value function $q_\\pi(s, a)$ is \n",
    "</br>\n",
    "</br>\n",
    "<font size=\"4\">\n",
    "$$\\begin{align}\n",
    "q_\\pi(s, a) = \\mathbb{E}_\\pi[R_{t+1} + \\gamma q_\\pi (S_{t+1}, A_{t+1})|S_t = s, A_t = a]\n",
    "\\end{align}$$\n",
    "</font>\n",
    "\n",
    "By Bellman equation, We can decompose estimation of value and action-value as more simpler, recursive subproblems.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimal value functions\n",
    "\n",
    "The final goal of RL is to find the policy that can achieve the highest cumulative reward in the long run. Therefore, in MDP the better policy can be defined as following:\n",
    "</br>\n",
    "</br>\n",
    "<font size=\"4\">\n",
    "$$\\begin{align}\n",
    "\\pi \\geq \\pi' \\text{  iff  } v_\\pi (s) \\geq v_{\\pi'} (s) \\text{  for  } \\forall s \\in \\mathcal{S}\n",
    "\\end{align}$$\n",
    "</font>\n",
    "\n",
    "There exists at least one **optimal policy** $\\pi_{*}$ that is better than all other policies.\n",
    "\n",
    "**Optimal policy** $\\pi_{*}$ achieves the **optimal value function** $v_*$ and **optiaml action-value function** $q_*$, which is \n",
    "\n",
    "</br>\n",
    "</br>\n",
    "<font size=\"4\">\n",
    "$$\\begin{align}\n",
    "v_*(s) = \\max_{\\pi} v_\\pi (s)\n",
    "\\end{align}$$\n",
    "</font>\n",
    "</br>\n",
    "<font size=\"4\">\n",
    "$$\\begin{align}\n",
    "q_*(s, a) = \\max_{\\pi} q_\\pi (s, a)\n",
    "\\end{align}$$\n",
    "</font>\n",
    "for all $s \\in \\mathcal{S}$ and $a \\in \\mathcal{A}$.\n",
    "\n",
    "Optimal value function and optimal action-value function can be connected as follows:\n",
    "</br>\n",
    "</br>\n",
    "<font size=\"4\">\n",
    "$$\\begin{align}\n",
    "v_*(s) = \\max_{a} q_* (s, a)\n",
    "\\end{align}$$\n",
    "</font>\n",
    "\n",
    "and bellman equation can be also applied to it. Bellman equation applied to $v_*$ and $q_*$ is called **Bellman optimality equation**.\n",
    "</br>\n",
    "</br>\n",
    "<font size=\"4\">\n",
    "$$\\begin{align}\n",
    "v_*(s) = \\mathbb{E}_\\pi[R_{t+1} + \\gamma v_* (S_{t+1})|S_t = s]\n",
    "\\end{align}$$\n",
    "</font>\n",
    "</br>\n",
    "<font size=\"4\">\n",
    "$$\\begin{align}\n",
    "q_*(s, a) = \\mathbb{E}_\\pi[R_{t+1} + \\gamma \\max_{a'} q_* (S_{t+1}, a')|S_t = s, A_t = a]\n",
    "\\end{align}$$\n",
    "</font>\n",
    "\n",
    "If we know optimal value function (or optimal action-value function) the MDP, it means 'MDP is solved' as we know the optimal policy $\\pi_*$ that can achieve maximum return. However, "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Difficulty of solving bellman optimality equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
