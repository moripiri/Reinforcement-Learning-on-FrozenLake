{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da7fe698",
   "metadata": {},
   "source": [
    "## Chapter 4: Model-Free Prediction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c487832a",
   "metadata": {},
   "source": [
    "Compared to **Dynamics Programming**, **Model-free prediction** algorithms aim to estimate the **value function of a certain policy** *without knowing the MDP*. Mainly, **Monte-Carlo Prediction** and **Temporal-Difference learning (TD-learning)** are known as a **model-free** prediction algorithms. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "966641ff",
   "metadata": {},
   "source": [
    "### Monte-Carlo Prediction\n",
    "\n",
    "According to Wikipedia, **Monte-Carlo (MC)** method is a broad class of algorithms that repeated random sampling to obtain numerical results. For example, the value of $\\pi$ can be approximated using a Monte Carlo method, by uniformly scattering number of points over the square. \n",
    "\n",
    "In Reinforcement Learning, it can be used to evaulate the value function of the policy $\\pi$ by sampling return of multiple episodes.\n",
    "</br>\n",
    "</br>\n",
    "<font size=\"4\">\n",
    "$$\\begin{align}\n",
    "v_\\pi(s) = \\mathbb{E}_\\pi[G_t |S_t = s]\n",
    "\\end{align}$$\n",
    "</font>\n",
    "where return $G_t = R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\cdots \\gamma^{T-1} R_T$. \n",
    "\n",
    "Monte-Carlo policy evaluation consider empirical mean return as value. \n",
    "\n",
    "After an episode end, we can update the value of the states visited in the episode. Value of the states can be updated by average of the returns following the first visits to the states (*First-Visit* MC), or every visits to the states (*Every-Visit* MC). \n",
    "\n",
    "\n",
    "Both first-visit and every-visit MC converge to true value function $v_\\pi$ with finite variance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ac3b47",
   "metadata": {},
   "source": [
    "### First-Visit and Every-Visit MC Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c5e998",
   "metadata": {},
   "source": [
    "### Temporal-Difference Learning\n",
    "\n",
    "**Temporal-Difference (TD)** learning is a model-free learning algorithm that learns by bootstrapping from the current estimate of the value function. \n",
    "\n",
    "</br>\n",
    "</br>\n",
    "<font size=\"4\">\n",
    "$$\\begin{align}\n",
    "v_\\pi(s) = \\mathbb{E}_\\pi[G_t |S_t = s]  & \\\\ = \\mathbb{E}_\\pi[\\sum_{k=0}^{T-1}\\gamma^k R_{t+k+1} |S_t = s] \\\\= \\mathbb{E}_\\pi[R_{t+1} + \\gamma \\sum_{k=0}^{T-2}\\gamma^{k} R_{t+k+2} |S_t = s] \\\\=\n",
    "\\mathbb{E}_\\pi [R_{t+1} + \\gamma v_\\pi(s_{t+1}) | S_t = s]\n",
    "\\end{align}$$\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0145a7cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6421609",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a182f4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl",
   "language": "python",
   "name": "rl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
