{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0996ec21",
   "metadata": {},
   "source": [
    "## Chapter 5: Model-Free Control"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff26565e",
   "metadata": {},
   "source": [
    "From the value function of unknown MDP estimated by model-free prediction algorithms, we can approximate optimal policies for the MDP. \n",
    "\n",
    "Model-free control algorithms can be divided into two groups: on-policy control and off-policy control.\n",
    "\n",
    "- On-policy control: Learn about policy $\\pi$ from experience sampled from $\\pi$\n",
    "\n",
    "- Off-policy control: Learn about policy from $\\pi$ from experience sampled from policy $\\mu \\neq \\pi$ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b20992",
   "metadata": {},
   "source": [
    "### On-policy Monte-Carlo Control\n",
    "\n",
    "The overall idea of model-free control is to proceed the same pattern as the  **Generalized Policy Iteration** -> iteration of policy evaluation and policy improvement. \n",
    "\n",
    "For policy evaluation algorithm, Monte-Carlo policy evaluation can be used, and for policy improvement algorithm, **greedy policy improvement** can be used.\n",
    "\n",
    "</br>\n",
    "</br>\n",
    "<font size=\"4\">\n",
    "$$\\begin{align}\n",
    "\\pi'(s) = \\text{argmax}_{a \\in \\mathcal{A}} Q(s, a)\n",
    "\\end{align}$$\n",
    "</font>\n",
    "\n",
    "To make monte-carlo control model-free, action-value function $Q(s, a)$ has to be used. \n",
    "\n",
    "#### $\\epsilon$-greedy policy improvement\n",
    "\n",
    "Policy improvement theorem (Chapter 3) guarantees greedy policy improvement assures improved policy $\\pi'$ is better or as good as the policy $\\pi$. However, it does not guarantee the convergence of $\\pi'$ to optimal policy due to lack of exploration. To ensure continual exploration, $\\epsilon$-greedy exploration can be considered.\n",
    "</br>\n",
    "</br>\n",
    "<font size=\"4\">\n",
    "$$ \\pi(a|s)=\\begin{cases}\n",
    "    \\frac{\\epsilon}{m} + 1-\\epsilon, & \\text{if $\\alpha^*=\\text{argmax}_{a \\in \\mathcal{A}}Q(s,a)$}.\\\\\n",
    "    \\frac{\\epsilon}{m}, & \\text{otherwise}.\n",
    "  \\end{cases}$$\n",
    "</font>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d0c8d53",
   "metadata": {},
   "source": [
    "### On-policy Temporal-Difference Control "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d406cbb7",
   "metadata": {},
   "source": [
    "### Off-policy Temporal-Difference Control"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl",
   "language": "python",
   "name": "rl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
