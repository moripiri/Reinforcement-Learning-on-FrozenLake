{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0baecdb",
   "metadata": {},
   "source": [
    "## Chapter 7: Policy Gradient Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a0b46c4",
   "metadata": {},
   "source": [
    "### Policy-based Reinforcement Learning\n",
    "\n",
    "In previous chapters, we focused on learning value function and generate policy from it. Yet there are RL algorithms that directly generate the policy from experience: **Policy-based RL**.\n",
    "\n",
    "In policy-based RL, we use parametrized policy by differentiable parameter $\\theta$. \n",
    "</br>\n",
    "</br>\n",
    "<font size=\"3\">\n",
    "$$\\begin{align}\n",
    "\\pi_\\theta(a|s) = \\mathbb{P}[a |s, \\theta]\n",
    "\\end{align}$$\n",
    "</font>\n",
    "\n",
    "Policy-based RL is effective in **high-dimensional** or **continuous action spaces**, and able to learn **stochastic policies**.\n",
    "\n",
    "\n",
    "#### Policy Gradient Theorem\n",
    "\n",
    "In policy gradient, policy parameter $\\theta$ is updated by some scalar performance measure $\\mathcal{J}(\\theta)$ with respect to the policy parameter. To maximize policy performance, their updates approximate **gradient ascent** in $\\mathcal{J}$:\n",
    "</br>\n",
    "</br>\n",
    "<font size=\"3\">\n",
    "$$\\begin{align}\n",
    "\\theta_{t+1} = \\theta_t + \\alpha \\nabla \\mathcal{J}(\\theta_t)\n",
    "\\end{align}$$\n",
    "</font>\n",
    "\n",
    "where $\\alpha$ is a step-size parameter..\n",
    "\n",
    "Then what can be the performance measure $\\mathcal{J}(\\theta)$ for MDP policy in finite episode?\n",
    "\n",
    "In episodic case trajectory $\\tau$. Then, we can define $\\mathcal{J}(\\theta)$ as its value.\n",
    "</br>\n",
    "</br>\n",
    "<font size=\"3\">\n",
    "$$\\begin{align}\n",
    "\\mathcal{J}(\\theta) = \\sum_{s \\in \\mathcal{S}} d^\\pi(s)v_\\pi(s) = \\sum_{s \\in \\mathcal{S}} d^\\pi(s) \\sum_{a \\in \\mathcal{A}} \\pi_\\theta(a|s) q_\\pi(s, a)\n",
    "\\end{align}$$\n",
    "</font>\n",
    "where $d^\\pi$ is the stationary distribution for Markov chain for $\\pi_\\theta$.\n",
    "\n",
    "Then gradient of $\\mathcal{J}$ can be reformatted as the following:\n",
    "</br>\n",
    "</br>\n",
    "<font size=\"3\">\n",
    "$$\\begin{align}\n",
    "\\nabla_\\theta \\mathcal{J}(\\theta) = \\nabla_\\theta \\sum_{s \\in \\mathcal{S}} d^\\pi(s) \\sum_{a \\in \\mathcal{A}} \\pi_\\theta(a|s) Q_\\pi(s, a) & \\\\\n",
    "\\propto \\sum_{s \\in \\mathcal{S}} d^\\pi(s) \\sum_{a \\in \\mathcal{A}} Q_\\pi(s, a) \\nabla_\\theta \\pi_\\theta(a|s) \\\\\n",
    " = \\sum_{s \\in \\mathcal{S}} d^\\pi(s) \\sum_{a \\in \\mathcal{A}} \\pi_\\theta(s, a) Q_\\pi(s, a) \\dfrac{\\nabla_\\theta \\pi_\\theta(a|s)}{\\pi_\\theta(a|s)} \\\\ \n",
    "= \\mathbb{E}_\\pi [Q_\\pi(s,a) \\nabla_\\theta \\log \\pi_\\theta(s,a)]\n",
    "\\end{align}$$\n",
    "</font>\n",
    "\n",
    "where $\\mathbb{E}_\\pi$ refers to $\\mathbb{E}_{s \\sim d_\\pi, a \\sim \\pi_\\theta}$ and $Q_\\pi$ means **true state-value** under policy $\\pi$. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "093d6294",
   "metadata": {},
   "source": [
    "### Monte-Carlo Policy Gradient\n",
    "\n",
    "\n",
    "As gradient of the policy performance measure $\\mathcal{J}$ can be expressed as the following:\n",
    "</br>\n",
    "</br>\n",
    "<font size=\"3\">\n",
    "$$\\begin{align}\n",
    "\\nabla_\\theta \\mathcal{J}(\\theta) = \\mathbb{E}_\\pi [q_\\pi(s,a) \\nabla_\\theta \\log \\pi_\\theta(s,a)]\n",
    "\\end{align}$$\n",
    "</font>\n",
    "\n",
    "we have to calculate true state-value $Q_\\pi$. \n",
    "\n",
    "Classical policy gradient algorithm **REINFORCE** uses **return** $G_t$ for $Q_\\pi$, as \n",
    "</br>\n",
    "</br>\n",
    "<font size=\"3\">\n",
    "$$\\begin{align}\n",
    "\\mathbb{E}_\\pi [G_t|s_t, a_t] = Q_\\pi (s_t, a_t)\n",
    "\\end{align}$$\n",
    "</font>\n",
    "\n",
    "To calculate return, policy update is done at the end of the every episode as the following.\n",
    "\n",
    "#### REINFORCE\n",
    "\n",
    "After generating an episode $s_0, a_0, r_0, ..., s_{T-1}, a_{T-1}, r_T$, following policy $\\pi_\\theta$,\n",
    "\n",
    "For each episode step $t = 0, 1, ... , T-1$\n",
    "</br>\n",
    "</br>\n",
    "<font size=\"3\">\n",
    "$$\\begin{align}\n",
    "G_t \\leftarrow \\sum_{k=t+1}^T \\gamma^{k-t-1}r_k \\\\ \n",
    "\\theta \\leftarrow \\theta + \\alpha \\gamma^t G_t \\nabla_\\theta \\log \\pi_\\theta (a_t|s_t)\n",
    "\\end{align}$$\n",
    "</font>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "189a03e4",
   "metadata": {},
   "source": [
    "### Actor-Critic Policy Gradient\n",
    "\n",
    "Yet the algorithm **REINFORCE** has a disadvantage in high gradient variance. \n",
    ".\n",
    "Thus, to reduce variance **critic** can be used instead of **return** to estimate $Q_\\pi$.\n",
    "</br>\n",
    "</br>\n",
    "<font size=\"3\">\n",
    "$$\\begin{align}\n",
    "q_\\phi (s, a) = Q_\\pi(s, a)\n",
    "\\end{align}$$\n",
    "</font>\n",
    "\n",
    "Therefore policy gradient is changed as\n",
    "</br>\n",
    "</br>\n",
    "<font size=\"3\">\n",
    "$$\\begin{align}\n",
    "\\nabla_\\theta \\mathcal{J}(\\theta) = \\mathbb{E}_\\pi [q_\\phi(s,a) \\nabla_\\theta \\log \\pi_\\theta(s,a)]\\\\\n",
    "\\theta \\leftarrow \\theta + \\alpha \\nabla_\\theta \\log \\pi_\\theta (a_t|s_t) q_\\phi(s_t, a_t)\n",
    "\\end{align}$$\n",
    "</font>\n",
    "\n",
    "And how we can update the critic $q_\\phi$? There exists many different methods:\n",
    "\n",
    "- Monte-Carlo evaluation\n",
    "- TD(0)\n",
    "- TD($\\lambda$)\n",
    "\n",
    "For example, if we use TD(0), then in timestep $t$ critic will be updated as\n",
    "</br>\n",
    "</br>\n",
    "<font size=\"3\">\n",
    "$$\\begin{align}\n",
    "\\phi \\leftarrow \\phi + \\beta (r_t + \\gamma q_\\phi(s_{t+1}, a_{t+1}) - q_\\phi(s_t, a_t)) \\nabla_\\phi q_\\phi(s_t, a_t)\n",
    "\\end{align}$$\n",
    "</font>\n",
    "\n",
    "#### Reducing a variance using baseline\n",
    "\n",
    "We can further reduce the variance of the policy gradient by subtracting baseline $B(s)$ from the policy gradient. \n",
    "\n",
    "A good baseline can be state value $V_\\pi$. \n",
    "</br>\n",
    "</br>\n",
    "<font size=\"3\">\n",
    "$$\\begin{align}\n",
    "A_\\pi(s, a) = Q_\\pi(s, a) - V_\\pi(s)\n",
    "\\end{align}$$\n",
    "</font>\n",
    "\n",
    "$A_\\pi$ is called **Advantage**. It can be intuitively thought as advantage of taking certain action in certain state $s$. \n",
    "\n",
    "Considering there are **states** that are more likely to achieve higher reward compared to other states (regardless of policy action), advantage can be better indicator for policy gradient.\n",
    "\n",
    "Furthermore, we can use TD target, approximating advantage to TD error.\n",
    "</br>\n",
    "</br>\n",
    "<font size=\"3\">\n",
    "$$\\begin{align}\n",
    "A_\\pi(s, a) \\doteq r + V_\\pi(s') - V_\\pi(s)\n",
    "\\end{align}$$\n",
    "</font>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl",
   "language": "python",
   "name": "rl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
